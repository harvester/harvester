# Title

Seeder HEP

## Summary

Seeder is a project to provision Harvester environments for use. It uses a series of K8s controllers and additional CNCF projects like `tinkerbell/boots` and `tinkerbell/rufio` to orchestrate the provisioning of Harvester across multiple nodes.

### Related Issues

[#2346](https://github.com/harvester/harvester/issues/2346)

## Motivation

We need an efficient way of provisioning Harvester on bare-metal servers to allow our dev teams to launch multiple versions of Harvester if required as part of the development or QA workflow.

Current methods of provisioning using an ISO image or PXE-based install require hand-holding to ensure that the infrastructure can be orchestrated and provisioned correctly.

### Goals

**Seeder** will be a separate project that can be deployed to a K8s cluster to manage the initial provisioning of multiple Harvester clusters.

### Non-goals [optional]


## Proposal

The moving parts of Seeder are as follows:

* `tinkerbell/boots`: `boots` as a conditional DHCP/PXE booting component in the Tinkerbell stack. It handles DHCP requests, assigns IPs, and serves up iPXE. It now supports a K8s backend for state management and allows users to define `Hardware` CRD objects to identify if a node needs to be assigned DHCP allocation or served up PXE.

* `tinkerbell/rufio`: `rufio` is a baseboard management controller. It can interact with most common hardware types using the `bmctool` kit to perform common operations like changing the boot device order and triggering the reboot of the machines. Using the `baseboardmanagement`, `bmctask`, and `bmcjob` CRD objects, users can interact with their bare-metal nodes via K8s objects.

* `seeder`: `seeder1 attempts to leverage `boots` and `rufio` to manage bare-metal nodes in an environment. `seeder` introduced three main CRDs and associated controllers. 
  * `AddressPools`: One of the critical tasks of Seeder is to perform IPAM. Traditionally, when using `boots`, a CIDR range needs to be allocated to conditionally serve out addresses when DHCP requests from match hardware objects are received. This is a challenge as we need to allocate large CIDR blocks, and non `boots` managed hardware or VMs also need associated entries as `Hardware` objects to trigger DHCP responses. Users can define multiple `AddressPools` objects and reference them in the Cluster definition. Seeder will allocate an address from the correct `AddressPools` and track the same as a status in the CRD object. Multiple `AddressPools` allow us to have non-continuous CIDR ranges if needed.
  * `Inventory`: `Inventory` is a wrapper around the `baseboardmanagement` CRD, along with some additional information needed by the `boots` and the Harvester installer. `Inventory` will be a long-lived object on the cluster and associated with an actual bare-metal node. `Inventory` will be related to a `Cluster` object in `seeder` and will be assigned to the cluster for the lifetime of the cluster object. For cluster deletion, the `Inventory` object will be freed and returned to the pool for reuse. The `Inventory` object manages an associated `baseboardmanagement` object, which performs tasks on the underlying bare-metal hardware. The inventory controller performs the following core tasks:
    * When creating an `Inventory` object, create an associated `baseboardManagement` object and then reconcile the state based on underlying `bmc` connectivity and power state reported by `rufio`. 
    * When an `Inventory` object is allocated to a cluster based on specific conditions, such as `Hardware` object generation condition, the controller creates a `bmcjob` object, which is used by `rufio` to trigger a reboot of the associated bare-metal node. This eventually triggers the DHCP/PXE boot server to kick-start Harvester installation.
    * When an `Inventory` object is freed from a cluster, the controller triggers another `bmcjob` to power off the bare-metal node.
  * `Cluster`: `Cluster` is an abstraction of an actual Harvester cluster running on a series of bare-metal nodes managed by associated `Inventory` objects. The cluster controller performs the following core tasks:
    * When creating a `Cluster`, allocate a `Harvester` VIP from the associated `AddressPool`.
    * Using a combination of the `Inventory` CRD spec and the `Harvester` CRD spec, generate a `Hardware` CRD, with valid `userdata`. This `userdata` is passed as kernel arguments by `boots` as part of the PXE booting workflow, and can be used to perform all Harvester configurations via kernel arguments.
    * Patch the `Inventory` status with correct conditions, allowing the `Inventory` controller to reboot the nodes via `bmcjobs` as described above.
    * Reconcile `Inventory` objects in the `Cluster` spec, allowing users to resize the cluster by adding or removing inventory as needed.

### User Stories

Once Seeder is deployed to a K8s cluster, the users should be able to quickly provision Harvester clusters.

#### Story 1

Create a Harvester Cluster.

By submitting a yaml file, a user will be able provision a fully functioning multi-node Harvester cluster available for use.

Sample yaml file:

```yaml
apiVersion: metal.harvesterhci.io/v1alpha1
kind: AddressPool
metadata:
  name: node-pool
  namespace: default
spec:
  cidr: "172.16.128.11/32"
  gateway: "172.16.128.1"
  netmask: "255.255.248.0"
---
apiVersion: metal.harvesterhci.io/v1alpha1
kind: AddressPool
metadata:
  name: node2-pool
  namespace: default
spec:
  cidr: "172.16.128.12/32"
  gateway: "172.16.128.1"
  netmask: "255.255.248.0"
---
apiVersion: metal.harvesterhci.io/v1alpha1
kind: AddressPool
metadata:
  name: vip-pool
  namespace: default
spec:
  cidr: "172.16.128.7/32"
  gateway: "172.16.128.1"
  netmask: "255.255.248.0"
---

apiVersion: v1
kind: Secret
metadata:
  name: node
  namespace: default
stringData:
  username: "ADMIN"
  password: "ADMIN"
---
apiVersion: metal.harvesterhci.io/v1alpha1
kind: Inventory
metadata:
  name: node
  namespace: default
spec:
  primaryDisk: "/dev/sda"
  managementInterfaceMacAddress: "0c:c4:7a:6b:84:20"
  baseboardSpec:
    connection:
      host: "172.16.1.53"
      port: 623
      insecureTLS: true
      authSecretRef:
        name: node
        namespace: default
---
apiVersion: v1
kind: Secret
metadata:
  name: node2
  namespace: default
stringData:
  username: "ADMIN"
  password: "ADMIN"
---
apiVersion: metal.harvesterhci.io/v1alpha1
kind: Inventory
metadata:
  name: node2
  namespace: default
spec:
  primaryDisk: "/dev/sda"
  managementInterfaceMacAddress: "0c:c4:7a:6b:80:d0"
  baseboardSpec:
    connection:
      host: "172.16.1.52"
      port: 623
      insecureTLS: true
      authSecretRef:
        name: node2
        namespace: default
---
apiVersion: metal.harvesterhci.io/v1alpha1
kind: Cluster
metadata:
  name: first
  namespace: default
spec:
  version: "v1.0.2"
  imageURL: "http://172.16.135.50:8080"
  clusterConfig:
    nameservers:
      - 172.16.128.1
  nodes:
    - inventoryReference:
        name: node
        namespace: default
      addressPoolReference:
        name: node-pool
        namespace: default
    - inventoryReference:
        name: node2
        namespace: default
      addressPoolReference:
        name: node2-pool
        namespace: default
  vipConfig:
    addressPoolReference:
      name: vip-pool
      namespace: default

```

Reconciled object state:
```shell
NAME                                  CLUSTERSTATUS         CLUSTERTOKEN       CLUSTERADDRESS
cluster.metal.harvesterhci.io/first   tinkHardwareCreated   iAnHlMWjrm0KF17Y   172.16.128.7

NAME                                    INVENTORYSTATUS      GENERATEDPASSWORD   ALLOCATEDNODEADDRESS
inventory.metal.harvesterhci.io/node    inventoryNodeReady   yE4d99nP2JIujlIw    172.16.128.11
inventory.metal.harvesterhci.io/node2   inventoryNodeReady   lv3xkopziz8r4Mbl    172.16.128.12

NAME                                           AGE
baseboardmanagement.bmc.tinkerbell.org/idrac   25h
baseboardmanagement.bmc.tinkerbell.org/node    25h
baseboardmanagement.bmc.tinkerbell.org/node2   25h

NAME                                     AGE
bmcjob.bmc.tinkerbell.org/node-reboot    25h
bmcjob.bmc.tinkerbell.org/node2-reboot   25h

NAME                                             AGE
bmctask.bmc.tinkerbell.org/node-reboot-task-0    25h
bmctask.bmc.tinkerbell.org/node-reboot-task-1    25h
bmctask.bmc.tinkerbell.org/node-reboot-task-2    25h
bmctask.bmc.tinkerbell.org/node2-reboot-task-0   25h
bmctask.bmc.tinkerbell.org/node2-reboot-task-1   25h
bmctask.bmc.tinkerbell.org/node2-reboot-task-2   25h

NAME                                           ADDRESSPOOLSTATUS   STARTADDRESS    LASTADDRESS     NETMASK
addresspool.metal.harvesterhci.io/node-pool    poolExhausted       172.16.128.11   172.16.128.11   255.255.248.0
addresspool.metal.harvesterhci.io/node2-pool   poolExhausted       172.16.128.12   172.16.128.12   255.255.248.0
addresspool.metal.harvesterhci.io/vip-pool     poolExhausted       172.16.128.7    172.16.128.7    255.255.248.0
(⎈ |seeder:seeder)➜  enhancements git:(seeder_hep) ✗ k get cluster,inventory,baseboardmanagement,bmcjob,bmctask,addresspool,hardware  -n default
NAME                                  CLUSTERSTATUS         CLUSTERTOKEN       CLUSTERADDRESS
cluster.metal.harvesterhci.io/first   tinkHardwareCreated   iAnHlMWjrm0KF17Y   172.16.128.7

NAME                                    INVENTORYSTATUS      GENERATEDPASSWORD   ALLOCATEDNODEADDRESS
inventory.metal.harvesterhci.io/node    inventoryNodeReady   yE4d99nP2JIujlIw    172.16.128.11
inventory.metal.harvesterhci.io/node2   inventoryNodeReady   lv3xkopziz8r4Mbl    172.16.128.12

NAME                                           AGE
baseboardmanagement.bmc.tinkerbell.org/idrac   25h
baseboardmanagement.bmc.tinkerbell.org/node    25h
baseboardmanagement.bmc.tinkerbell.org/node2   25h

NAME                                     AGE
bmcjob.bmc.tinkerbell.org/node-reboot    25h
bmcjob.bmc.tinkerbell.org/node2-reboot   25h

NAME                                             AGE
bmctask.bmc.tinkerbell.org/node-reboot-task-0    25h
bmctask.bmc.tinkerbell.org/node-reboot-task-1    25h
bmctask.bmc.tinkerbell.org/node-reboot-task-2    25h
bmctask.bmc.tinkerbell.org/node2-reboot-task-0   25h
bmctask.bmc.tinkerbell.org/node2-reboot-task-1   25h
bmctask.bmc.tinkerbell.org/node2-reboot-task-2   25h

NAME                                           ADDRESSPOOLSTATUS   STARTADDRESS    LASTADDRESS     NETMASK
addresspool.metal.harvesterhci.io/node-pool    poolExhausted       172.16.128.11   172.16.128.11   255.255.248.0
addresspool.metal.harvesterhci.io/node2-pool   poolExhausted       172.16.128.12   172.16.128.12   255.255.248.0
addresspool.metal.harvesterhci.io/vip-pool     poolExhausted       172.16.128.7    172.16.128.7    255.255.248.0

NAME                            STATE
hardware.tinkerbell.org/node
hardware.tinkerbell.org/node2 
```


### User Experience In Detail

1. User submits a yaml spec to provision a Harvester cluster.

2. User can currently see the **BMC console** to view the installation status.

3. The cluster can be accessed via the **Cluster VIP** available in the **Cluster Status**.

4. Each node can be accessed using the address and generated password in the **Inventory Status**.

### API changes

No API changes needed to core Harvester.

Currently seeder is a standalone project, and CRDs are the only way to interact with the controllers.

## Design

### Implementation Overview

Current implementation is available [here](https://github.com/harvester/seeder).

### Test plan

A lot of integration tests already exist in the code base to simulate and test most of the reconcile functionality.

### Upgrade strategy

N/A

## Note [optional]

N/A