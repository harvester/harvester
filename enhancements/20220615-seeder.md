# Title

Seeder HEP

## Summary

Seeder is a project to provision Harvester environments for use. It uses a series of K8s controllers and additional CNCF projects like `tinkerbell/boots` and `tinkerbell/rufio` to orchestrate the provisioning of Harvester across multiple nodes.

### Related Issues

[#2346](https://github.com/harvester/harvester/issues/2346)

## Motivation

We need an efficient way of provisioning Harvester on baremetal to allow our dev teams to be able to launch multiple versions of Harvester if needed as part of the development / QA workflow.

Current means of provisoning using ISO or PXE based install need a bit of hand holding to ensure that the infra can be orchestrated and provisioned correctly.

### Goals

Seeder will be a seperate project that can be deployed to a k8s cluster to manage the initial provisioning of multiple Harvester clusters.

### Non-goals [optional]


## Proposal

The moving parts of Seeder are as follows:

* `tinkerbell/boots`: boots is a conditional DHCP/PXE booting component in the tinkerbell stack. It now supports a K8s backend for state management, and allows users to define `Hardware` CRD objects which can be used to identify if a node needs to be served a DHCP allocation, and/ or also PXE boot the same. 

* `tinkerbell/rufio`: rufio is a baseboardmanagement controller. It can interact with most common hardware types using the bmctool kit to perform common operations like change boot device order and trigger reboot of the machines. Using `Baseboardmanagement`, `BMCTask` and `BMCJob` CRD objects, users can interact with their metal nodes via K8s objects.

* `seeder`: seeder attempts to leverage `boots` and `rufio` to manage metal nodes in an environment. There are three main CRD's and associated controllers introduced by seeder:
  * `AddressPools`: One of the key tasks of Seeder is to perform IPAM. Traditionally when using `boots` it needs to be allocated a CIDR range to conditionally serve out addresses when DHCP requests from match hardware objects are received. This is a challenge as we need to allocate entire large CIDR blocks, and non `boots` managed hardware / VM's also need associated entries as `Hardware` objects to trigger DHCP responses. Users can define multiple AddressPool objects and reference them in the Cluster definition. Seeder will allocate an address from the correct AddressPool and track the same as a status in the CRD object. Multiple Address pools allow us to have non-continuous CIDR ranges if needed.
  * `Inventory`: Inventory is a wrapper around the `BaseboardManagement` CRD, along with some additional information needed by the `boots` and the Harvester installer. Inventory will be a long lived object on the cluster, and will be associated with an actual metal node. `Inventory` will be associated with a `Cluster` object in seeder, and will be assigned to the cluster for the lifetime of the cluster object. On cluster deletion the Inventory object will be freed and returned back to the pool for reuse. The `Inventory` object manages an associated `BaseboardManagement` object, which can be used to perform tasks on the underlying metal hardware. The inventory controller performs the following core tasks:
    * On creation of `Inventory` object, create an associated `BaseboardManagement` object and then reconciles the state based on underlying bmc connectivity and power state reported by `rufio`. 
    * When `Inventory` object is allocated to a cluster, based on specific conditions, such as `Hardware` object generation condition, the controller creates a `BMCJob` object, which is used by `rufio` to trigger reboot of the associated metal node. This eventually triggers DHCP/PXE boot serve to kick start Harvester installation.
    * When `Inventory` object is freed from a cluster, the controller triggers another `BMCJob` to power off the metal node.
  * `Cluster`: Cluster is an abstraction of an actual Harvester cluster running on a series of baremetal nodes managed by associated `Inventory` objects. The cluster controller performs the following core tasks:
    * On creation of a Cluster, allocate a `Harvester` VIP from the associated AddressPool.
    * Using a combination of `Inventory` CRD spec and `Harvester` CRD spec, generate a `Hardware` CRD, with a valid userdata. This userdata is passed as kernel arguments by `boots` as part of the PXE booting workflow, and can be used to perform all Harvester configuration via kernel arguments.
    * Patch the `Inventory` status with correct conditions, allowing `Inventory` controller to trigger the reboot of the nodes via `BMCJobs` as describe above. 
    * Reconcile `Inventory` objects in `Cluster` spec, allowing users to resize the cluster by add / remove inventory as needed.

### User Stories
Once Seeder is deployed to a k8s cluster, the users should be able to quickly provision Harvester clusters.

#### Story 1
Create a Harvester Cluster

By submitting a yaml file, a user will be able to have a full functioning multi-node harvester cluster available for use.

Sample yaml file:
```yaml
apiVersion: metal.harvesterhci.io/v1alpha1
kind: AddressPool
metadata:
  name: node-pool
  namespace: default
spec:
  cidr: "172.16.128.11/32"
  gateway: "172.16.128.1"
  netmask: "255.255.248.0"
---
apiVersion: metal.harvesterhci.io/v1alpha1
kind: AddressPool
metadata:
  name: node2-pool
  namespace: default
spec:
  cidr: "172.16.128.12/32"
  gateway: "172.16.128.1"
  netmask: "255.255.248.0"
---
apiVersion: metal.harvesterhci.io/v1alpha1
kind: AddressPool
metadata:
  name: vip-pool
  namespace: default
spec:
  cidr: "172.16.128.7/32"
  gateway: "172.16.128.1"
  netmask: "255.255.248.0"
---

apiVersion: v1
kind: Secret
metadata:
  name: node
  namespace: default
stringData:
  username: "ADMIN"
  password: "ADMIN"
---
apiVersion: metal.harvesterhci.io/v1alpha1
kind: Inventory
metadata:
  name: node
  namespace: default
spec:
  primaryDisk: "/dev/sda"
  managementInterfaceMacAddress: "0c:c4:7a:6b:84:20"
  baseboardSpec:
    connection:
      host: "172.16.1.53"
      port: 623
      insecureTLS: true
      authSecretRef:
        name: node
        namespace: default
---
apiVersion: v1
kind: Secret
metadata:
  name: node2
  namespace: default
stringData:
  username: "ADMIN"
  password: "ADMIN"
---
apiVersion: metal.harvesterhci.io/v1alpha1
kind: Inventory
metadata:
  name: node2
  namespace: default
spec:
  primaryDisk: "/dev/sda"
  managementInterfaceMacAddress: "0c:c4:7a:6b:80:d0"
  baseboardSpec:
    connection:
      host: "172.16.1.52"
      port: 623
      insecureTLS: true
      authSecretRef:
        name: node2
        namespace: default
---
apiVersion: metal.harvesterhci.io/v1alpha1
kind: Cluster
metadata:
  name: first
  namespace: default
spec:
  version: "v1.0.2"
  imageURL: "http://172.16.135.50:8080"
  clusterConfig:
    nameservers:
      - 172.16.128.1
  nodes:
    - inventoryReference:
        name: node
        namespace: default
      addressPoolReference:
        name: node-pool
        namespace: default
    - inventoryReference:
        name: node2
        namespace: default
      addressPoolReference:
        name: node2-pool
        namespace: default
  vipConfig:
    addressPoolReference:
      name: vip-pool
      namespace: default

```

Reconciled object state:
```shell
NAME                                  CLUSTERSTATUS         CLUSTERTOKEN       CLUSTERADDRESS
cluster.metal.harvesterhci.io/first   tinkHardwareCreated   iAnHlMWjrm0KF17Y   172.16.128.7

NAME                                    INVENTORYSTATUS      GENERATEDPASSWORD   ALLOCATEDNODEADDRESS
inventory.metal.harvesterhci.io/node    inventoryNodeReady   yE4d99nP2JIujlIw    172.16.128.11
inventory.metal.harvesterhci.io/node2   inventoryNodeReady   lv3xkopziz8r4Mbl    172.16.128.12

NAME                                           AGE
baseboardmanagement.bmc.tinkerbell.org/idrac   25h
baseboardmanagement.bmc.tinkerbell.org/node    25h
baseboardmanagement.bmc.tinkerbell.org/node2   25h

NAME                                     AGE
bmcjob.bmc.tinkerbell.org/node-reboot    25h
bmcjob.bmc.tinkerbell.org/node2-reboot   25h

NAME                                             AGE
bmctask.bmc.tinkerbell.org/node-reboot-task-0    25h
bmctask.bmc.tinkerbell.org/node-reboot-task-1    25h
bmctask.bmc.tinkerbell.org/node-reboot-task-2    25h
bmctask.bmc.tinkerbell.org/node2-reboot-task-0   25h
bmctask.bmc.tinkerbell.org/node2-reboot-task-1   25h
bmctask.bmc.tinkerbell.org/node2-reboot-task-2   25h

NAME                                           ADDRESSPOOLSTATUS   STARTADDRESS    LASTADDRESS     NETMASK
addresspool.metal.harvesterhci.io/node-pool    poolExhausted       172.16.128.11   172.16.128.11   255.255.248.0
addresspool.metal.harvesterhci.io/node2-pool   poolExhausted       172.16.128.12   172.16.128.12   255.255.248.0
addresspool.metal.harvesterhci.io/vip-pool     poolExhausted       172.16.128.7    172.16.128.7    255.255.248.0
(⎈ |seeder:seeder)➜  enhancements git:(seeder_hep) ✗ k get cluster,inventory,baseboardmanagement,bmcjob,bmctask,addresspool,hardware  -n default
NAME                                  CLUSTERSTATUS         CLUSTERTOKEN       CLUSTERADDRESS
cluster.metal.harvesterhci.io/first   tinkHardwareCreated   iAnHlMWjrm0KF17Y   172.16.128.7

NAME                                    INVENTORYSTATUS      GENERATEDPASSWORD   ALLOCATEDNODEADDRESS
inventory.metal.harvesterhci.io/node    inventoryNodeReady   yE4d99nP2JIujlIw    172.16.128.11
inventory.metal.harvesterhci.io/node2   inventoryNodeReady   lv3xkopziz8r4Mbl    172.16.128.12

NAME                                           AGE
baseboardmanagement.bmc.tinkerbell.org/idrac   25h
baseboardmanagement.bmc.tinkerbell.org/node    25h
baseboardmanagement.bmc.tinkerbell.org/node2   25h

NAME                                     AGE
bmcjob.bmc.tinkerbell.org/node-reboot    25h
bmcjob.bmc.tinkerbell.org/node2-reboot   25h

NAME                                             AGE
bmctask.bmc.tinkerbell.org/node-reboot-task-0    25h
bmctask.bmc.tinkerbell.org/node-reboot-task-1    25h
bmctask.bmc.tinkerbell.org/node-reboot-task-2    25h
bmctask.bmc.tinkerbell.org/node2-reboot-task-0   25h
bmctask.bmc.tinkerbell.org/node2-reboot-task-1   25h
bmctask.bmc.tinkerbell.org/node2-reboot-task-2   25h

NAME                                           ADDRESSPOOLSTATUS   STARTADDRESS    LASTADDRESS     NETMASK
addresspool.metal.harvesterhci.io/node-pool    poolExhausted       172.16.128.11   172.16.128.11   255.255.248.0
addresspool.metal.harvesterhci.io/node2-pool   poolExhausted       172.16.128.12   172.16.128.12   255.255.248.0
addresspool.metal.harvesterhci.io/vip-pool     poolExhausted       172.16.128.7    172.16.128.7    255.255.248.0

NAME                            STATE
hardware.tinkerbell.org/node
hardware.tinkerbell.org/node2 
```


### User Experience In Detail

1) User submits a yaml spec to provision a Harvester cluster.

2) User can currently watch the BMC console to view the installation status.

3) The cluster can be access via the Cluster VIP available in Cluster status.

4) Each node can be accessed using the address and generated password in the Inventory status.

### API changes
No API changes needed to core harvester.

Currently seeder is a standalone project, and CRDs are the only way to interact with the controllers.

## Design

### Implementation Overview

Current implementation is available [here](https://github.com/harvester/seeder).

### Test plan

A lot of integration tests already exist in the code base to simulate and test most of the reconcile functionality.

### Upgrade strategy

N/A

## Note [optional]

N/A