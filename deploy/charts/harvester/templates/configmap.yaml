apiVersion: v1
kind: ConfigMap
metadata:
  name: vip
data:
  enabled: {{ .Values.service.vip.enabled | quote }}
  serviceType: {{ .Values.service.vip.type }}
  ip: {{ .Values.service.vip.ip | quote }}
  mode: {{ .Values.service.vip.mode }}
  hwAddress: {{ .Values.service.vip.hwAddress | quote }}
  loadBalancerIP: {{ .Values.service.vip.loadBalancerIP | quote }}

---
kind: ConfigMap
apiVersion: v1
metadata:
  name: harvester-helpers
  labels:
{{ include "harvester.labels" . | indent 4 }}
data:
  promote.sh: |-
    {{`KUBECTL="/host/$(readlink /host/var/lib/rancher/rke2/bin)/kubectl"
    YQ="/host/usr/bin/yq"
    ROLE_LABELS="rke.cattle.io/control-plane-role=true rke.cattle.io/etcd-role=true"
    ETCD_ONLY=false
    if [[ -n "$1" && $1 == "rke.cattle.io/etcd-role=true" ]]; then
      ETCD_ONLY=true
      ROLE_LABELS=$1
    fi

    case $ROLE_LABELS in
      "rke.cattle.io/control-plane-role=true rke.cattle.io/etcd-role=true" | "rke.cattle.io/etcd-role=true")
    ;;
      *)
      echo "ROLE $ROLE_LABELS is not supported."
      exit 1
    ;;
    esac

    get_machine_from_node() {
      $KUBECTL get node $HARVESTER_PROMOTE_NODE_NAME -o jsonpath='{.metadata.annotations.cluster\.x-k8s\.io/machine}'
    }

    # Wait for rancher-webhook ready. It's default to one replica.
    # Otherwise lebeling capi resources later might fail.
    $KUBECTL rollout status --watch=true deployment rancher-webhook -n cattle-system
    # https://github.com/rancher/webhook/blob/436e359b136b633cb1a6fa7cdedbed4d74821bdb/pkg/server/server.go#L114
    sleep 20

    CUSTOM_MACHINE=$(get_machine_from_node)
    until [ -n "$CUSTOM_MACHINE" ]
    do
      echo Waiting for custom machine label of $HARVESTER_PROMOTE_NODE_NAME ...
      sleep 2
      CUSTOM_MACHINE=$(get_machine_from_node)
    done

    until $KUBECTL get machines.cluster.x-k8s.io $CUSTOM_MACHINE -n fleet-local &> /dev/null
    do
      echo Waiting for custom machine $CUSTOM_MACHINE...
      sleep 2
    done

    PLAN_SECRET="${CUSTOM_MACHINE}-machine-plan"
    until $KUBECTL get secret $PLAN_SECRET -n fleet-local &> /dev/null
    do
      echo Waiting for machine plan of $CUSTOM_MACHINE...
      sleep 2
    done

    until $KUBECTL get rkebootstraps.rke.cattle.io "${CUSTOM_MACHINE}" -n fleet-local &> /dev/null
    do
      echo Waiting for bootstrap object of $CUSTOM_MACHINE...
      sleep 2
    done
    `}}

    VIP=$($KUBECTL get configmap vip -n harvester-system -o=jsonpath='{.data.ip}')
    cat > /host/etc/rancher/rke2/config.yaml.d/90-harvester-server.yaml <<EOF
    cni: multus,canal
    cluster-cidr: {{ .Values.promote.clusterPodCIDR }}
    service-cidr: {{ .Values.promote.clusterServiceCIDR }}
    cluster-dns: {{ .Values.promote.clusterDNS }}
    tls-san:
      - $VIP
    audit-policy-file: /etc/rancher/rke2/config.yaml.d/92-harvester-kube-audit-policy.yaml
    EOF

    {{`
    # Disable snapshot-controller related charts because we manage them in Harvester.
    # RKE2 enables these charts by default after v1.25.7 (https://github.com/rancher/rke2/releases/tag/v1.25.7%2Brke2r1)
    cat > /host/etc/rancher/rke2/config.yaml.d/40-disable-charts.yaml <<EOF
    disable:
    - rke2-snapshot-controller
    - rke2-snapshot-controller-crd
    - rke2-snapshot-validation-webhook
    EOF

    # make sure we should not have any related label/taint on the node
    if [[ $ETCD_ONLY == false ]]; then
      found=$($KUBECTL get node $HOSTNAME -o yaml | $YQ '.spec.taints[] | select (.effect == "NoSchedule" and .key == "node-role.kubernetes.io/etcd=true") | .effect')
      if [[ -n $found ]]
      then
        $KUBECTL taint nodes $HOSTNAME node-role.kubernetes.io/etcd=true:NoExecute-
      fi
      $KUBECTL label --overwrite nodes $HOSTNAME node-role.harvesterhci.io/witness-
    fi

    # For how to promote nodes, see: https://github.com/rancher/rancher/issues/36480#issuecomment-1039253499
    $KUBECTL label --overwrite -n fleet-local machines.cluster.x-k8s.io $CUSTOM_MACHINE $ROLE_LABELS
    $KUBECTL label --overwrite -n fleet-local machines.cluster.x-k8s.io $CUSTOM_MACHINE cluster.x-k8s.io/control-plane=true
    $KUBECTL label --overwrite -n fleet-local secret $PLAN_SECRET $ROLE_LABELS
    $KUBECTL label --overwrite -n fleet-local rkebootstraps.rke.cattle.io $CUSTOM_MACHINE $ROLE_LABELS

    kickout_longhorn_node()
    {
      target=$1
      found=$($KUBECTL get nodes.longhorn.io -n longhorn-system |grep -q $target && echo true || echo false)
      if [[ $found == true ]]; then
        echo "Found longhorn node $target, kicking it out..."
        $KUBECTL delete nodes.longhorn.io $target -n longhorn-system
      fi
    }

    while true
    do
      if [[ $ETCD_ONLY == true ]]; then
        ETCD_STATE=$($KUBECTL get node $HOSTNAME -o go-template=$'{{index .metadata.labels "node-role.kubernetes.io/etcd"}}\n' || true)

        if [ "$ETCD_STATE" = "true" ]; then
          $KUBECTL taint nodes $HOSTNAME node-role.kubernetes.io/etcd=true:NoExecute --overwrite
          $KUBECTL patch managedchart harvester -n fleet-local --type=json -p='[{"op":"replace", "path":"/spec/values/replicas", "value": 2}]'
          $KUBECTL patch managedchart harvester -n fleet-local --type=json -p='[{"op":"replace", "path":"/spec/values/webhook/replicas", "value": 2}]'
          $KUBECTL annotate --overwrite deployment rancher -n cattle-system management.cattle.io/scale-available="2"
          kickout_longhorn_node $HOSTNAME
          break
        fi

      else
        CONTROL_PLANE=$($KUBECTL get node $HOSTNAME -o go-template=$'{{index .metadata.labels "node-role.kubernetes.io/control-plane"}}\n' || true)

        if [ "$CONTROL_PLANE" = "true" ]; then
          break
        fi
      fi
      echo Waiting for promotion...
      sleep 2
    done
    `}}
  cpu-manager.sh: |-
    function cleanup_cpu_manager_state() {
      if [ -f "$CPU_MANAGER_STATE_FILE" ]; then
        mv "$CPU_MANAGER_STATE_FILE" "${CPU_MANAGER_STATE_FILE}.old"
        echo "File $CPU_MANAGER_STATE_FILE has been renamed to ${CPU_MANAGER_STATE_FILE}.old"
      else
        echo "File $CPU_MANAGER_STATE_FILE does not exist."
      fi
    }

    function manage_service() {
      local service=$1

      echo "Stopping ${service}."
      if ! chroot $HOST_DIR systemctl stop $service; then
        echo "Error: failed to stop ${service}."
        exit 11
      fi
      echo "Stopped ${service}."

      cleanup_cpu_manager_state

      echo "Starting ${service}."
      if ! chroot $HOST_DIR systemctl start $service; then
        echo "Error: failed to start ${service}."
        exit 12
      fi
      echo "Started ${service}."
    }

    function wait_for_label() {
      local node_name=$1
      local expect_label_value=$2
      local interval=5
      local elapsed=0
      local start_time=$(date +%s)

      while true; do
        local current_time=$(date +%s)
        local elapsed=$((current_time - start_time))

        if [ $elapsed -ge $WAIT_LABEL_TIMEOUT ]; then
          echo "Error: timeout, elapsed ${elapsed}s"
          exit 21
        fi

        if ! labels=$($KUBECTL get node "$node_name" --show-labels); then
          echo "Warning: failed to get labels for $node_name, retrying in 3s..."
          sleep 3
          continue
        fi
        if echo "$labels" | grep -q "cpumanager=$expect_label_value"; then
          echo "End update cpu-manager-policy"
          exit 0
        fi
        echo "Value in label cpumanager is not $expect_label_value, wait ${interval}s..."
        sleep $interval
      done
    }

    echo "Start update cpu-manager-policy option..."
    KUBECTL="$HOST_DIR/$(readlink $HOST_DIR/var/lib/rancher/rke2/bin)/kubectl"
    CPU_MANAGER_CONFIG_FILE="$HOST_DIR/etc/rancher/rke2/config.yaml.d/99-z01-harvester-cpu-manager.yaml"
    STATIC_POLICY="static"
    NONE_POLICY="none"
    CPU_MANAGER_STATE_FILE="$HOST_DIR/var/lib/kubelet/cpu_manager_state"
    NODE_NAME="$1"
    NODE_POLICY="$2"
    EXIT_CODE=0

    if [ "$NODE_POLICY" != "$STATIC_POLICY" ] && [ "$NODE_POLICY" != "$NONE_POLICY" ]; then
      echo "Error: invalid cpu-manager-policy $NODE_POLICY"
      exit 1
    fi

    if ! $KUBECTL get node "$NODE_NAME" --show-labels | grep -q "cpumanager="; then
      echo "Error: There is no label cpumanager in node $NODE_NAME."
      exit 2
    fi

    printf 'kubelet-arg+:\n- "cpu-manager-policy=%s"' "$NODE_POLICY" > $CPU_MANAGER_CONFIG_FILE

    if chroot $HOST_DIR systemctl is-active --quiet rke2-server; then
      manage_service "rke2-server"
    elif chroot $HOST_DIR systemctl is-active --quiet rke2-agent; then
      manage_service "rke2-agent"
    else
      echo "Error: Neither rke2-server nor rke2-agent are running."
      exit 3
    fi

    if [ "$NODE_POLICY" = "$STATIC_POLICY" ]; then
      EXPECT_LABEL_VALUE="true"
    else
      EXPECT_LABEL_VALUE="false"
    fi

    wait_for_label "$NODE_NAME" "$EXPECT_LABEL_VALUE"
